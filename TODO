[Done] Add download folder specification in config file
[Done] Add category and page number specification in config file
[Done] (use FastAPI and Uvicorn to run Python API server, relatively high performance) Decide whether to have a seperate program to serve public api or make the crawler itself to serve public api (might not have a good performance), or just no public api at all
[Done] Add whether to download all pages of a illustration or just the first page, or just ignore the illustration if it has multiple pages
[Done] (always use original) Add image quality specification in config file (original, large, medium)
[Done] Add store mode that only stores image info (id, title, author, tags, url) in database (sqlite, or other sql/nosql serverless database)
[Done] Port ConfigParser to ConfigUpdater to avoid config file/comment corruption (need to add .value to all getters)
[Done] (always in background and crawl periodically) Decide whether to make the crawler runs freshly and periodically (i.e. using a crontab to update images) or make the crawler stays in background and runs periodically (specify fetch time interval in config)
[Done] Format all prints to use proper logging module
[Done] Build API server that contains: config that could use local image cache/remote urls, reload config, specify reverse proxy of pixiv image; allowing users to randomize image according to given tags/r18/ai_type/author/id; allowing users to specify certain db id to lookup certain picture; having two serve modes that could either return json to user or directly return image file to user (might be unnecessary)
[Done] Add limit to the number of images/tags/authors to crawl/return
[Done] Add ability to crawl from more than one ranking mode

Add ability to crawl from a specific date or a specific period of time in the past